---
title: "Assignment#4 - ML regression and BL regression models "
author: "Douglas Barley, Ethan Haley, Isabel Magnus, John Mazon, Vinayak Kamath, Arushi Arora"
date: "11/07/2021"
output:
  html_document: 
    toc: true
    toc-title: "Assignment 4 - Multiple linear regression and Binary logistic regression models  "
    toc_depth: 4
    toc_float:
      collapsed: false
      smooth_scroll: false
    theme: darkly
    highlight: pygments
  pdf_document: default
---

```{r setup, include=FALSE}
#knitr::opts_chunk$set(echo = TRUE, warning = F)
#if (!require("ggplot2",character.only = TRUE)) (install.packages("ggplot2",dep=TRUE))
#if (!require("knitr",character.only = TRUE)) (install.packages("knitr",dep=TRUE))
#if (!require("xtable",character.only = TRUE)) (install.packages("xtable",dep=TRUE))
#if (!require("dplyr",character.only = TRUE)) (install.packages("dplyr",dep=TRUE))
#if (!require("stringr",character.only = TRUE)) (install.packages("stringr",dep=TRUE))
#if (!require("Hmisc",character.only = TRUE)) (install.packages("Hmisc",dep=TRUE))
#if (!require("ClusterR",character.only = TRUE)) (install.packages("ClusterR",dep=TRUE))
#if (!require("cluster",character.only = TRUE)) (install.packages("cluster",dep=TRUE))
#if (!require("lattice",character.only = TRUE)) (install.packages("lattice",dep=TRUE))

library(ggplot2)
library(knitr)
library(xtable)
library(dplyr)
library(stringr)
library(tidyverse)
library(dplyr)
library(ROCR)
library(Hmisc)
library(corrplot)
library(MASS)
library(caret)
library(tidyr)
library(data.table)
require(data.table)
require(car)
require(corrgram)
require(ggplot2)
#library(ClusterR)
library(cluster)
library(lattice)
```

 DATA 621 – Business Analytics and Data Mining
 

# Overview  
In this homework assignment, you will explore, analyze and model a data set containing approximately 8000 
records  representing  a  customer  at  an  auto  insurance  company.  Each  record  has  two  response  variables.  The 
first  response  variable, TARGET_FLAG, is a 1 or a 0. A “1” means that the person was in a car crash. A zero 
means that the person was not in a car crash. The second response variable is TARGET_AMT. This value is zero 
if the person did not crash their car. But if they did crash their car, this number will be a value greater than zero. 
 
Your objective is to build multiple linear regression and binary logistic regression models on the training data 
to predict the probability that a person will crash their car and also the amount of money it will cost if the person 
does crash their car. You can only use the variables given to you (or variables that you derive from the variables 
provided). 

- **INDEX**:  Identification Variable (do not use)
- **TARGET_FLAG**: Was Car in a crash? 1=YES 0=NO None
- **TARGET_AMT**: If car was in a crash, what was the cost None
- **AGE**: Age of Driver Very young people tend to be risky. Maybe very old people also.
- **BLUEBOOK**: Value of Vehicle Unknown effect on probability of collision, but probably effect the payout if there is a crash
- **CAR_AGE**: Vehicle Age Unknown effect on probability of collision, but probably effect the payout if there is a crash
- **CAR_TYPE**: Type of Car Unknown effect on probability of collision, but probably effect the payout if there is a crash
- **CAR_USE**: Vehicle Use Commercial vehicles are driven more, so might increase probability of collision
- **CLM_FREQ**: # Claims (Past 5 Years) The more claims you filed in the past, the more you are likely to file in the future
- **EDUCATION**: Max Education Level Unknown effect, but in theory more educated people tend to drive more safely
- **HOMEKIDS**: # Children at Home Unknown effect
- **HOME_VAL**: Home Value In theory, home owners tend to drive more responsibly
- **INCOME**: Income In theory, rich people tend to get into fewer crashes
- **JOB**: Job Category In theory, white collar jobs tend to be safer
- **KIDSDRIV**: # Driving Children When teenagers drive your car, you are more likely to get into crashes
- **MSTATUS**: Marital Status In theory, married people drive more safely
- **MVR_PTS**: Motor Vehicle Record Points If you get lots of traffic tickets, you tend to get into more crashes
- **OLDCLAIM**: Total Claims (Past 5 Years) If your total payout over the past five years was high, this suggests future payouts will be high
- **PARENT1**: Single Parent Unknown effect
- **RED_CAR**: A Red Car Urban legend says that red cars (especially red sports cars) are more risky. Is that true?
- **REVOKED**: License Revoked (Past 7 Years) If your license was revoked in the past 7 years, you probably are a more risky driver.
- **SEX**: Gender Urban legend says that women have less crashes then men. Is that true?
- **TIF**: Time in Force People who have been customers for a long time are usually more safe.
- **TRAVTIME**: Distance to Work Long drives to work usually suggest greater risk
- **URBANICITY**: Home/Work Area Unknown
- **YOJ**: Years on Job People who stay at a job for a long time are usually more safe

## 1. Data Exploration
Describe the size and the variables in the  insurance training data set. Consider that too much detail will cause a 
manager to lose interest while too little detail will make the manager consider that you aren’t doing your job. Some 
suggestions are given below. Please do NOT treat this as a check list of things to do to complete the assignment. 
You should have your own thoughts on what to tell the boss. These are just ideas. 
a. Mean / Standard Deviation / Median 
b. Bar Chart or Box Plot of the data 
c. Is the data correlated to the target variable (or to other variables?) 
d. Are any of the variables missing and need to be imputed “fixed”?





## 2. Data Preparation
Describe how you have transformed the data by changing the original variables or creating new variables. If you 
did transform the data or create new variables, discuss why you did this. Here are some possible transformations. 
 
a. Fix missing values (maybe with a Mean or Median value) 
b. Create flags to suggest if a variable was missing 
c. Transform data by putting it into buckets 
d. Mathematical transforms such as log or square root (or use Box-Cox) 
e. Combine variables (such as ratios or adding or multiplying) to create new variables

### Load datasets

```{r reading data}
ins_train_df <- read.csv("https://raw.githubusercontent.com/johnm1990/msds-621/main/insurance_training_data.csv")
ins_eval_df <- read.csv("https://raw.githubusercontent.com/johnm1990/msds-621/main/insurance-evaluation-data.csv")
```

### inspect values

```{r}
summary(ins_train_df)
```

```{r}
#before whole list of columns telling class, now showing measure

ins_train_df <- as.data.frame(lapply(ins_train_df, gsub, pattern='z_', replacement=''))
ins_eval_df <- as.data.frame(lapply(ins_eval_df, gsub, pattern='z_', replacement=''))
cols.num <- c("TARGET_FLAG",
"TARGET_AMT","AGE","YOJ","TRAVTIME","TIF","CLM_FREQ",
"MVR_PTS","CAR_AGE","KIDSDRIV","HOMEKIDS")

ins_train_df[cols.num] <- sapply(ins_train_df[cols.num],as.numeric)
ins_eval_df[cols.num] <- sapply(ins_eval_df[cols.num],as.numeric)

cols.fac <- c("EDUCATION", "SEX", "CAR_TYPE", "JOB", "CAR_USE", "URBANICITY")

ins_train_df[cols.fac] <- sapply(ins_train_df[cols.fac],factor)
ins_eval_df[cols.fac] <- sapply(ins_eval_df[cols.fac],factor)


#do not need, the function already handled
#ins_train_df <- lapply(ins_train_df, gsub, pattern='$', replacement='')
#ins_eval_df <- lapply(ins_eval_df, gsub, pattern='$', replacement='')

#ins_train_df <- lapply(ins_train_df, gsub, pattern=',', replacement='')
#ins_eval_df <- lapply(ins_eval_df, gsub, pattern=',', replacement='')

ins_eval_df$SEX <- factor(ins_eval_df$SEX)
ins_train_df$SEX <- factor(ins_train_df$SEX)
ins_eval_df$JOB <- factor(ins_eval_df$JOB)
ins_train_df$JOB <- factor(ins_train_df$JOB)
ins_eval_df$CAR_USE <- factor(ins_eval_df$CAR_USE)
ins_train_df$CAR_USE <- factor(ins_train_df$CAR_USE)
ins_eval_df$CAR_TYPE <- factor(ins_eval_df$CAR_TYPE)
ins_train_df$CAR_TYPE <- factor(ins_train_df$CAR_TYPE)
ins_eval_df$URBANICITY <- factor(ins_eval_df$URBANICITY)
ins_train_df$URBANICITY <- factor(ins_train_df$URBANICITY)

ins_eval_df$EDUCATION <- factor(ins_eval_df$EDUCATION,
                                   levels = c("<High School", "High School",
                                               "Bachelors", "Masters", "PhD"),
                                    ordered = T)
ins_train_df$EDUCATION <- factor(ins_train_df$EDUCATION,
                                   levels = c("<High School", "High School",
                                               "Bachelors", "Masters", "PhD"),
                                    ordered = T)
```



#### Some columns should be numbers but aren't

```{r}
#print first six values
head(ins_train_df$BLUEBOOK)
typeof(ins_train_df$BLUEBOOK)
```


```{r}
head(ins_train_df$INCOME)
typeof(ins_train_df$INCOME)
```

```{r}
head(ins_train_df$HOME_VAL)
typeof(ins_train_df$HOME_VAL)
```

```{r}
head(ins_train_df$OLDCLAIM)
typeof(ins_train_df$OLDCLAIM)
```

#### convert strings to integers, filling blanks as 0

#need to check for decimals, convert to integer.No decimals.
#these four variables need to add them to ins_train_numeric
```{r}

head(ins_train_df$INCOME)

#change it to as.numeric from original (as.integer) ***overwrites existing character type
dollars <- function(n){
  as.numeric(paste0('0', str_remove_all(n, '[,$]')))  # add leading zero for blanks
}
dollars(ins_train_df$INCOME[1:11])
```

```{r}
#converting to dollars of blue blook

ins_eval_df$BLUEBOOK = dollars(ins_eval_df$BLUEBOOK)
ins_train_df$BLUEBOOK = dollars(ins_train_df$BLUEBOOK)
ins_eval_df$HOME_VAL = dollars(ins_eval_df$HOME_VAL)
ins_train_df$HOME_VAL = dollars(ins_train_df$HOME_VAL)
ins_eval_df$INCOME = dollars(ins_eval_df$INCOME)
ins_train_df$INCOME = dollars(ins_train_df$INCOME)
ins_eval_df$OLDCLAIM = dollars(ins_eval_df$OLDCLAIM)
ins_train_df$OLDCLAIM = dollars(ins_train_df$OLDCLAIM)
```

#### Inspect more features

```{r}
#change to head
ins_train_df$PARENT1[1:11]
ins_train_df$MSTATUS[1:11]
ins_train_df$SEX[1:11]
ins_train_df$EDUCATION[1:11]
ins_train_df$JOB[1:11]
ins_train_df$CAR_USE[1:11]
ins_train_df$CAR_TYPE[1:11]
ins_train_df$REVOKED[1:11]
ins_train_df$URBANICITY[1:11]
ins_train_df$RED_CAR[1:11]

table(ins_train_df$URBANICITY)
```
#### Turn boolean strings into booleans

```{r}
#not necessary
#changes to logical type

table(ins_train_df$REVOKED)

ins_train_df$REVOKED = tolower(ins_train_df$REVOKED)=='yes'
ins_eval_df$REVOKED = tolower(ins_eval_df$REVOKED)=='yes'
ins_train_df$PARENT1 = tolower(ins_train_df$PARENT1)=='yes'
ins_eval_df$PARENT1 = tolower(ins_eval_df$PARENT1)=='yes'
ins_train_df$RED_CAR = tolower(ins_train_df$RED_CAR)=='yes'
ins_eval_df$RED_CAR = tolower(ins_eval_df$RED_CAR)=='yes'
ins_train_df$MSTATUS = tolower(ins_train_df$MSTATUS)=='yes'
ins_eval_df$MSTATUS = tolower(ins_eval_df$MSTATUS)=='yes'
```



```{r}
summary(ins_train_df)
ins_train_df$INCOME <- as.numeric(ins_train_df$INCOME)
typeof(ins_train_df$INCOME)
head(ins_train_df$INCOME)
```






```{r}
#x include all numeric column **leave until we see correlation
#summaries for mean

ins_train_numeric <- ins_train_df %>% 
  dplyr::select(where(is.numeric))
#we will see what type of variable they are with str
str(ins_train_df)
kable(sapply(ins_train_numeric, function(ins_train_numeric) c( "Stand dev" = sd(ins_train_numeric), 
                         "Mean"= mean(ins_train_numeric,na.rm=TRUE),
                         "n" = length(ins_train_numeric),
                         "Median" = median(ins_train_numeric,na.rm = TRUE),
                         "CoeffofVariation" = sd(ins_train_numeric)/mean(ins_train_numeric,na.rm=TRUE),
                         "Minimum" = min(ins_train_numeric),
                         "Maximum" = max(ins_train_numeric),
                         "Upper Quantile" = quantile(ins_train_numeric,1,na.rm = TRUE),
                         "LowerQuartile" = quantile(ins_train_numeric,0,na.rm = TRUE)
                    )
)
)

```



```{r}
###summary statistics
#ignore all missing values, provides our SD for all numeric
summary(ins_train_df)
sapply(ins_train_numeric, sd, na.rm=TRUE)
#####sapply(ins_train_numeric, hist, na.rm=TRUE)

ins_train_numeric %>% gather() %>% head()

ggplot(gather(ins_train_numeric), aes(value)) + 
    geom_histogram(bins = 10) + 
    facet_wrap(~key, scales = 'free_x')
#have a deeper look, potentially need to transform, huge variance
hist(ins_train_df$TARGET_AMT)
summary(ins_train_df$TARGET_AMT)
var(ins_train_df$TARGET_AMT)
var(log(ins_train_df$TARGET_AMT+1))
#benefit of log transform, 
#we added because there a lot of 0's in Target_AMT, we added 1

#log of 0 undefined, thats why 0, need to find a better transformation that handles zeros well

#have a deeper look, potentially need to transform, huge variance
hist(ins_train_df$INCOME)
summary(ins_train_df$INCOME)
var(ins_train_df$INCOME)
var(log(ins_train_df$INCOME+1))
#table(ins_train_df$INCOME)

#have a deeper look, potentially need to transform, huge variance (extremely compressed)
#important to note log0 is undefinded, in order to transform needed to change to 1
#age, yoj,car_age many misssing values
#we are focusing on the ones with missing values first

hist(ins_train_df$OLDCLAIM)
summary(ins_train_df$OLDCLAIM)
var(ins_train_df$OLDCLAIM)
var(log(ins_train_df$OLDCLAIM+1))
#table(ins_train_df$OLDCLAIM)

summary(ins_train_df)

# ggplot(gather(ins_train_numeric, cols, value), aes(x = value)) + 
#        geom_histogram(binwidth = 20) + facet_grid(.~cols)


##correlation matrix
ins_train_numeric.rcorr = rcorr(as.matrix(ins_train_numeric))
ins_train_numeric.rcorr

#a lot of NA's for the three variables, Correlation take all values in the column and associate them

ins_train_numeric.cor = cor(ins_train_numeric, use = "complete.obs")
corrplot(ins_train_numeric.cor)


#homekids looks like good group for imputing ages
#table(ins_train_df$HOMEKIDS)
#incoming needs to be clustered for imputing



#create five clusters of income
kmeans.re <- kmeans(ins_train_df$INCOME, centers = 5)
table(kmeans.re$cluster)
ins_train_df$inc_clusters <- kmeans.re$cluster
#table(ins_train_df$INCOME)

#we are creating groups based on homekids, replace missing value
ins_train_df <- ins_train_df %>% 
             group_by(HOMEKIDS) %>% 
            mutate(AGE= ifelse(is.na(AGE), mean(AGE, na.rm=TRUE), AGE))


#we are creating groups based on car_age, replace missing value
ins_train_df <- ins_train_df %>% 
             group_by(inc_clusters) %>% 
            mutate(CAR_AGE= ifelse(is.na(CAR_AGE), mean(CAR_AGE, na.rm=TRUE), CAR_AGE))

#we are creating groups based on homekids, replace missing value
#no group value is highly correlated by YOJ
ins_train_df <- ins_train_df %>% 
  mutate(YOJ= ifelse(is.na(YOJ), mean(YOJ, na.rm=TRUE), YOJ))

summary(ins_train_df)

#correlations between variables we highly suspect

cor_inctarget <- cor.test(ins_train_df$INCOME, ins_train_df$TARGET_AMT)
cor_inctarget

cor_hovaltarget <- cor.test(ins_train_df$HOME_VAL, ins_train_df$TARGET_AMT)
cor_hovaltarget

cor_cfreqtarget<- cor.test(ins_train_df$CLM_FREQ, ins_train_df$TARGET_AMT)
cor_cfreqtarget


cor_mptstarget<- cor.test(ins_train_df$MVR_PTS, ins_train_df$TARGET_AMT)
cor_mptstarget

cor_travtarget<- cor.test(ins_train_df$TRAVTIME, ins_train_df$TARGET_AMT)
cor_travtarget
#the weaker the correlation the higher the p value

#ttests
#most P values significantly low
t_inctarget <- t.test(ins_train_df$INCOME~ ins_train_df$TARGET_FLAG)
t_inctarget

t_hovaltarget <- t.test(ins_train_df$HOME_VAL~ ins_train_df$TARGET_FLAG)
t_hovaltarget

t_cfreqtarget<- t.test(ins_train_df$CLM_FREQ~ ins_train_df$TARGET_FLAG)
t_cfreqtarget


t_mptstarget<- t.test(ins_train_df$MVR_PTS~ ins_train_df$TARGET_FLAG)
t_mptstarget

t_travtarget<- t.test(ins_train_df$TRAVTIME~ ins_train_df$TARGET_FLAG)
t_travtarget

#percentages is better because same scale for both graphs

histogram(~ INCOME | TARGET_FLAG, data = ins_train_df)
histogram(~ HOME_VAL | TARGET_FLAG, data = ins_train_df)
histogram(~ CLM_FREQ | TARGET_FLAG, data = ins_train_df)
histogram(~ MVR_PTS | TARGET_FLAG, data = ins_train_df)

#checking the distribution, flag 0 means no car crashes, target 1 means car crashes, we are checking distribution of income between both

```



## 3. Build the Models

### Models predicting TARGET_AMT


```{r}
#inspecting the TARGET_AMT variable
boxplot(ins_train_df$TARGET_AMT)
summary(ins_train_df$TARGET_AMT)
```  
#### Model 1- All variables discussed above or literally ALL?
```{r}
#Building model using all features.

lm_ins_train_df <- lm(TARGET_AMT ~ KIDSDRIV + AGE + HOMEKIDS + YOJ + INCOME +
                    PARENT1 +HOME_VAL + MSTATUS + SEX + EDUCATION + JOB + TRAVTIME +
                    CAR_USE + BLUEBOOK + TIF + CAR_TYPE + RED_CAR + OLDCLAIM + CLM_FREQ +
                    REVOKED + MVR_PTS + CAR_AGE + URBANICITY
                    , data = ins_train_df)

summary(lm_ins_train_df)

```  

#### Model 2- Selectively chosen variables

```{r}
#Building model Selectively chosen features.

lms_ins_train_df <- lm(TARGET_AMT ~ KIDSDRIV +   MSTATUS + CAR_USE + BLUEBOOK +  CAR_TYPE 
                      + OLDCLAIM + CLM_FREQ + REVOKED + MVR_PTS + URBANICITY
                    , data = ins_train_df)

summary(lms_ins_train_df)

```  

```{r}
par(mfrow=c(1,2))
plot(lms_ins_train_df$residuals ~ lms_ins_train_df$fitted.values, main="New Reduced Var Model")
abline(h = 0)
plot(lm_ins_train_df$residuals ~ lm_ins_train_df$fitted.values, main="Orignal Model All Vars")
abline(h = 0)
```

## 4. Select Models



 

